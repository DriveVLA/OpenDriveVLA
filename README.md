# OpenDriveVLA: Towards End-to-end Autonomous Driving with  Large Vision Language Action Model

<h3 align="center">
  <a href="https://drivevla.github.io/">Project Page</a> |
  <a href="">arXiv</a>
</h3>

![](assets/drivevla-ModelArc.jpg)

## Overview ‚ú®

- [Todo List](#todo-list-)
- [News](#news-)
- [Getting Started](#getting-started-)
- [Citation](#citation)
- [Related Resources](#related-resources)

## TODO List üìÖ

We will release the model code and checkpoints soon. Stay tuned! üî•

- [x] Release environment setup
- [ ] Release inference code
- [ ] Release checkpoints
- [ ] Release training code

## News üì¢

- **`2025/03/28`** We release the environment setup of OpenDriveVLA.
  - To make the dependencies of our OpenDriveVLA model [[mmcv](https://github.com/open-mmlab/mmcv) & [mmdet3d](https://github.com/open-mmlab/mmdetection3d)] compatible with [PyTorch 2.1.2](https://pytorch.org/) and support [Transformers](https://github.com/huggingface/transformers) and [Deepspeed](https://github.com/deepspeedai/DeepSpeed), we selected specific versions and enhanced the source code accordingly. The resulting customized libraries are available in the `third_party` folder.

## Getting Started üåü

1. [OpenDriveVLA Environment Installation](docs/1_INSTALL.md)

## Citation üìù

If you find our project useful for your research, please consider citing our paper and codebase with the following BibTeX:

```bibtex
```

## Acknowledgement ü§ù

- [Transformers](https://github.com/huggingface/transformers)
- [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT)
- [Qwen2.5](https://github.com/QwenLM/Qwen2.5)
- [UniAD](https://github.com/OpenDriveLab/UniAD)
- [mmcv](https://github.com/open-mmlab/mmcv)
- [mmdet3d](https://github.com/open-mmlab/mmdetection3d)
- [GPT-Driver](https://github.com/PointsCoder/GPT-Driver)
- [Hint-AD](https://github.com/Robot-K/Hint-AD)
- [TOD3Cap](https://github.com/jxbbb/TOD3Cap)
